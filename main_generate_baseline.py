#!/usr/bin/env python3
# main_generate_baseline.py
"""
- PromptAdapter æ”¯æŒ 4 ä¸ª Prompt å®éªŒç»´åº¦ï¼š
    * exp_family = quality / complexity / num_demos / diversity / none
    * condition / num_demos / diversity_mode æ§åˆ¶å…·ä½“å˜ä½“
"""

import os
import time
import json
import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Any, List, Tuple, Optional

from tqdm import tqdm
from openai import OpenAI

from baseline import read_problems, write_jsonl
from execution import check_correctness  # ç›®å‰ baseline å†…ä¸ä½¿ç”¨ï¼Œä½†ä¿ç•™å¯¼å…¥ä»¥å…¼å®¹æ—§ä»£ç 

API_KEY = os.getenv("OPENAI_API_KEY") or os.getenv("DASHSCOPE_API_KEY")
BASE_URL = os.getenv("BASE_URL", "https://dashscope.aliyuncs.com/compatible-mode/v1")
MODEL_NAME = os.getenv("MODEL_NAME", "qwen3-8b")
DEFAULT_PROBLEM_FILE = "HumanEval.jsonl"

DEFAULT_MAX_SAMPLES = 80
DEFAULT_EXP_FAMILY = "none"          # å¯é€‰: none / quality / complexity / num_demos / diversity
DEFAULT_CONDITION = None             # æ¯”å¦‚ "clean" / "wrong_demo" / "simple" / "detailed" ç­‰
DEFAULT_NUM_DEMOS = 0                # num_demos å®éªŒæ—¶çš„é»˜è®¤ K
DEFAULT_DIVERSITY_MODE = "low"       # diversity å®éªŒé»˜è®¤: low / high
DEFAULT_OUTPUT_FILE = "baseline_A2_default.jsonl"

GLOBAL_THINKING_MODE = False
DEFAULT_WORKERS = 10
DEFAULT_TEMPERATURE = 1
CALL_TIMEOUT = 120

def call_with_retry(func, retries: int = 2, base_delay: float = 1.0):
    for attempt in range(retries + 1):
        try:
            return func()
        except Exception as e:
            if attempt == retries:
                raise
            time.sleep(base_delay * (2 ** attempt) + 0.1)


def extract_usage(resp) -> Dict[str, int]:
    usage = getattr(resp, "usage", None)
    if usage is None and isinstance(resp, dict):
        usage = resp.get("usage", {})

    def as_dict(o):
        if o is None:
            return {}
        if isinstance(o, dict):
            return o
        try:
            return o.__dict__
        except Exception:
            return {}

    u = as_dict(usage)
    prompt = int(u.get("prompt_tokens", u.get("input_tokens", 0) or 0))
    completion = int(u.get("completion_tokens", u.get("output_tokens", 0) or 0))
    total = int(u.get("total_tokens", prompt + completion))
    return {
        "prompt_tokens": prompt,
        "completion_tokens": completion,
        "total_tokens": total,
    }


def _canonical_solution_or_placeholder(problem: Dict[str, Any]) -> str:
    sol = problem.get("canonical_solution")
    if isinstance(sol, str) and sol.strip():
        return sol.strip()
    entry = problem.get("entry_point", "solution")
    return (
        f"def {entry}(*args, **kwargs):\n"
        f"    # Reference implementation not available; placeholder used in prompt only.\n"
        f"    raise NotImplementedError\n"
    )


def _make_wrong_solution(problem: Dict[str, Any]) -> str:
    sol = problem.get("canonical_solution")
    if not isinstance(sol, str) or not sol.strip():
        return _canonical_solution_or_placeholder(problem)

    lines = sol.splitlines()
    header_line = None
    indent = ""
    for line in lines:
        if line.strip().startswith("def "):
            header_line = line.rstrip()
            indent = line[: len(line) - len(line.lstrip())]
            break
    if not header_line:
        return _canonical_solution_or_placeholder(problem)

    body = (
        f"{indent}    # NOTE: intentionally wrong demo implementation\n"
        f"{indent}    return None\n"
    )
    return header_line + "\n" + body


def _pick_irrelevant_demo(task_id: str, problems: Dict[str, Dict[str, Any]], task_order: List[str]) -> Tuple[str, Dict[str, Any]]:
    if not task_order:
        task_order = sorted(problems.keys())
    if task_id not in task_order:
        task_order = sorted(problems.keys())
    idx = task_order.index(task_id)
    other_idx = (idx + 1) % len(task_order)
    other_tid = task_order[other_idx]
    return other_tid, problems[other_tid]


def _pick_demo_tasks(
    problems: Dict[str, Dict[str, Any]],
    task_order: List[str],
    k: int,
    exclude_task: Optional[str] = None,
) -> List[Tuple[str, Dict[str, Any]]]:
    if k <= 0:
        return []
    if not task_order:
        task_ids = sorted(problems.keys())
    else:
        task_ids = list(task_order)

    demo_ids: List[str] = []
    for tid in task_ids:
        if exclude_task is not None and tid == exclude_task:
            continue
        demo_ids.append(tid)
        if len(demo_ids) >= k:
            break

    return [(tid, problems[tid]) for tid in demo_ids]

# diversity ç”¨çš„å¤šç§æ¨¡æ¿
DIVERSITY_TEMPLATES: List[Dict[str, str]] = [
    {
        "system": "You are an expert Python developer.",
        "prompt_prefix": "è¯·æ ¹æ®ä»¥ä¸‹æè¿°å®ç°ä¸€ä¸ªå®Œæ•´çš„ Python å‡½æ•°ï¼š\n\n",
    },
    {
        "system": "You are a senior Python engineer who writes clean and efficient code.",
        "prompt_prefix": "Read the following problem description and implement the required function in Python:\n\n",
    },
    {
        "system": "You are a meticulous Python programmer who always passes all unit tests.",
        "prompt_prefix": "Implement the following Python function so that it satisfies all (possibly hidden) tests:\n\n",
    },
    {
        "system": "You are a Python tutor. You write simple but correct Python functions.",
        "prompt_prefix": "Please write a correct Python function according to the following instructions:\n\n",
    },
]


def _build_base_messages(system_text: str, user_prompt_text: str) -> List[Dict[str, str]]:
    user_content = (
        f"{user_prompt_text}\n\n"
        "âš ï¸è¯·ä»…è¾“å‡ºå®Œæ•´çš„å‡½æ•°å®šä¹‰ï¼ˆä»¥ def å¼€å¤´ï¼‰ï¼Œä¸è¦è§£é‡Šæˆ–æ·»åŠ æ³¨é‡Šã€‚\n"
    )
    return [
        {"role": "system", "content": system_text},
        {"role": "user", "content": user_content},
    ]


def build_messages(
    task_id: str,
    problem: Dict[str, Any],
    all_problems: Dict[str, Dict[str, Any]],
    cfg: Dict[str, Any],
) -> Tuple[List[Dict[str, str]], Dict[str, Any]]:
    exp_family: str = cfg.get("exp_family", "none") or "none"
    condition: Optional[str] = cfg.get("condition")
    num_demos: int = int(cfg.get("num_demos", 0) or 0)
    diversity_mode: Optional[str] = cfg.get("diversity_mode")
    task_order: List[str] = cfg.get("task_order") or sorted(all_problems.keys())

    system_default = "You are an expert Python developer."
    base_prompt_text = f"è¯·æ ¹æ®ä»¥ä¸‹æè¿°å®ç°ä¸€ä¸ªå®Œæ•´çš„ Python å‡½æ•°ï¼š\n\n{problem['prompt']}"
    prompt_meta: Dict[str, Any] = {
        "exp_family": exp_family,
        "condition": condition,
        "prompt_variant": None,
        "num_demos": 0,
        "diversity_mode": None,
        "prompt_template_id": 0,
    }

    # 1. Prompt Quality
    if exp_family == "quality":
        variant = condition or "clean"
        prompt_meta["prompt_variant"] = variant

        if variant == "clean":
            msgs = _build_base_messages(system_default, base_prompt_text)
            return msgs, prompt_meta

        elif variant == "wrong_demo":
            wrong_code = _make_wrong_solution(problem)
            user_text = (
                "ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹ä»»åŠ¡åŠå…¶è§£ç­”ï¼ˆæ³¨æ„ï¼šç¤ºä¾‹å®ç°æ˜¯æ•…æ„å†™é”™çš„ï¼Œåªç”¨äºå±•ç¤ºæ ¼å¼ï¼‰ï¼š\n\n"
                "ã€ç¤ºä¾‹é¢˜ç›®ã€‘\n"
                f"{problem['prompt']}\n\n"
                "ã€é”™è¯¯ç¤ºä¾‹ä»£ç ã€‘\n"
                f"{wrong_code}\n\n"
                "ç°åœ¨è¯·ä½ å¿½ç•¥ä¸Šè¿°é”™è¯¯å®ç°ï¼Œä»…å°†å…¶ä½œä¸ºæ ¼å¼å‚è€ƒã€‚\n"
                "è¯·ä¸ºä¸‹é¢çš„æ­£å¼ä»»åŠ¡é‡æ–°ç¼–å†™**æ­£ç¡®**çš„ Python å‡½æ•°å®ç°ï¼š\n\n"
                f"{problem['prompt']}"
            )
            msgs = _build_base_messages(system_default, user_text)
            return msgs, prompt_meta

        elif variant == "irrelevant_demo":
            other_tid, other_prob = _pick_irrelevant_demo(task_id, all_problems, task_order)
            demo_code = _canonical_solution_or_placeholder(other_prob)
            user_text = (
                "ä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹ç¼–ç¨‹ä»»åŠ¡åŠå…¶è§£ç­”ï¼ˆä¸å½“å‰ä»»åŠ¡æ— å…³ï¼Œä»…ä½œæ ¼å¼å‚è€ƒï¼‰ï¼š\n\n"
                "ã€ç¤ºä¾‹é¢˜ç›®ã€‘\n"
                f"{other_prob['prompt']}\n\n"
                "ã€ç¤ºä¾‹ Python ä»£ç ã€‘\n"
                f"{demo_code}\n\n"
                "ç°åœ¨è¯·ä½ å®Œæˆä¸‹é¢çœŸæ­£çš„ç›®æ ‡ä»»åŠ¡ï¼š\n\n"
                f"{problem['prompt']}"
            )
            msgs = _build_base_messages(system_default, user_text)
            return msgs, prompt_meta

        elif variant == "bad_instruction":
            system_text = (
                "You are a helpful assistant. You may output explanations in natural language "
                "before or after the code, and you do not need to strictly ensure the code can run."
            )
            user_text = (
                f"è¯·æ ¹æ®ä»¥ä¸‹æè¿°å®ç°ä¸€ä¸ª Python å‡½æ•°ï¼Œå¹¶å¯ä»¥åœ¨ä»£ç å‰ååŠ å…¥ä½ çš„è§£é‡Šï¼š\n\n{problem['prompt']}\n\n"
                "ä½ å¯ä»¥åœ¨è¾“å‡ºä¸­åŠ å…¥è‡ªç„¶è¯­è¨€è¯´æ˜ã€‚"
            )
            msgs = _build_base_messages(system_text, user_text)
            return msgs, prompt_meta

        else:
            prompt_meta["prompt_variant"] = "clean(fallback)"
            msgs = _build_base_messages(system_default, base_prompt_text)
            return msgs, prompt_meta

    # 2. Prompt Complexity
    if exp_family == "complexity":
        variant = condition or "original"
        prompt_meta["prompt_variant"] = variant

        base = problem["prompt"].strip()
        if variant == "simple":
            lines = [ln for ln in base.splitlines() if ln.strip()]
            if len(lines) > 2:
                truncated = "\n".join(lines[:2])
            else:
                truncated = base
            user_text = f"è¯·å®ç°ä¸€ä¸ª Python å‡½æ•°ï¼Œæ»¡è¶³ä»¥ä¸‹ç®€è¦è¦æ±‚ï¼š\n\n{truncated}"
            msgs = _build_base_messages(system_default, user_text)
            return msgs, prompt_meta

        elif variant == "detailed":
            entry = problem.get("entry_point", "solution")
            extra = (
                "\n\nè¯·ç‰¹åˆ«æ³¨æ„ï¼š\n"
                f"- å‡½æ•°åå¿…é¡»ä¸º `{entry}`ã€‚\n"
                "- å°½å¯èƒ½è¦†ç›–è¾¹ç•Œæ¡ä»¶ä¸å¼‚å¸¸è¾“å…¥ã€‚\n"
                "- é¿å…ä½¿ç”¨å…¨å±€å˜é‡ï¼Œä¿æŒå®ç°ç®€æ´æ¸…æ™°ã€‚"
            )
            user_text = (
                "è¯·æ ¹æ®ä»¥ä¸‹è¾ƒä¸ºè¯¦ç»†çš„æè¿°ï¼Œå®ç°ä¸€ä¸ªå¥å£®çš„ Python å‡½æ•°ï¼š\n\n"
                f"{base}{extra}"
            )
            msgs = _build_base_messages(system_default, user_text)
            return msgs, prompt_meta

        else:  # original
            user_text = base_prompt_text
            msgs = _build_base_messages(system_default, user_text)
            return msgs, prompt_meta

    # 3. Number of Demonstrations
    if exp_family == "num_demos":
        prompt_meta["prompt_variant"] = "num_demos"
        prompt_meta["num_demos"] = int(num_demos)

        if num_demos <= 0:
            msgs = _build_base_messages(system_default, base_prompt_text)
            return msgs, prompt_meta

        demos = _pick_demo_tasks(all_problems, task_order, num_demos, exclude_task=task_id)

        demo_blocks: List[str] = []
        for idx, (tid, demo_prob) in enumerate(demos, start=1):
            demo_code = _canonical_solution_or_placeholder(demo_prob)
            block = (
                f"### ç¤ºä¾‹ {idx}\n"
                "ã€ç¤ºä¾‹é¢˜ç›®ã€‘\n"
                f"{demo_prob['prompt']}\n\n"
                "ã€ç¤ºä¾‹ Python è§£ç­”ã€‘\n"
                f"{demo_code}\n"
            )
            demo_blocks.append(block)

        demos_text = "\n\n".join(demo_blocks)
        user_text = (
            "ä¸‹é¢ç»™å‡ºè‹¥å¹²ç¤ºä¾‹ç¼–ç¨‹ä»»åŠ¡åŠå…¶ Python è§£ç­”ï¼Œè¯·å…ˆé˜…è¯»è¿™äº›ç¤ºä¾‹ï¼Œç„¶åå®Œæˆæœ€åçš„ç›®æ ‡ä»»åŠ¡ã€‚\n\n"
            f"{demos_text}\n\n"
            "=== ç°åœ¨è¯·å®Œæˆä¸‹é¢çš„ç›®æ ‡ä»»åŠ¡ ===\n\n"
            f"{problem['prompt']}"
        )
        msgs = _build_base_messages(system_default, user_text)
        return msgs, prompt_meta

    # 4. Prompt Diversity
    if exp_family == "diversity":
        mode = (diversity_mode or "low").lower()
        prompt_meta["prompt_variant"] = "diversity"
        prompt_meta["diversity_mode"] = mode

        if mode == "high":
            if not task_order:
                task_order = sorted(all_problems.keys())
            if task_id in task_order:
                pos = task_order.index(task_id)
            else:
                pos = 0
            template_id = pos % len(DIVERSITY_TEMPLATES)
        else:
            template_id = 0

        tmpl = DIVERSITY_TEMPLATES[template_id]
        prompt_meta["prompt_template_id"] = template_id

        user_text = f"{tmpl['prompt_prefix']}{problem['prompt']}"
        msgs = _build_base_messages(tmpl["system"], user_text)
        return msgs, prompt_meta
    prompt_meta["prompt_variant"] = "default"
    msgs = _build_base_messages(system_default, base_prompt_text)
    return msgs, prompt_meta
def make_process_one(
    model_name: str,
    temperature: float,
    thinking_mode: bool,
    save_raw_flag: Optional[Dict[str, Any]],
    problems: Dict[str, Dict[str, Any]],
    prompt_cfg: Dict[str, Any],
):
    def process_one(entry: Tuple[str, Dict[str, Any]]):
        task_id, problem = entry
        client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

        messages, prompt_meta = build_messages(task_id, problem, problems, prompt_cfg)

        try:
            t0 = time.time()
            resp = call_with_retry(
                lambda: client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    temperature=temperature,
                    extra_body={"enable_thinking": bool(thinking_mode)},
                    timeout=CALL_TIMEOUT,
                )
            )
            gen_time = time.time() - t0

            content_raw = resp.choices[0].message.content.strip()
            usage = extract_usage(resp)

            result = {
                "task_id": task_id,
                "completion": content_raw,
                "tokens": {
                    "prompt_tokens": usage["prompt_tokens"],
                    "completion_tokens": usage["completion_tokens"],
                    "total_tokens": usage["total_tokens"],
                },
                "thinking_mode_enabled": bool(thinking_mode),
                "generation_time": float(gen_time),
                "note": "ok",
                "prompt_config": prompt_meta,
            }

            if save_raw_flag is not None and not save_raw_flag.get("saved", False):
                try:
                    with open("debug_last_response.json", "w", encoding="utf-8") as f:
                        json.dump(
                            {
                                "task_id": task_id,
                                "messages": messages,
                                "response": content_raw,
                                "prompt_config": prompt_meta,
                            },
                            f,
                            ensure_ascii=False,
                            indent=2,
                        )
                    save_raw_flag["saved"] = True
                except Exception:
                    pass

            return result, gen_time

        except Exception as e:
            err_result = {
                "task_id": task_id,
                "completion": "",
                "tokens": {
                    "prompt_tokens": 0,
                    "completion_tokens": 0,
                    "total_tokens": 0,
                },
                "thinking_mode_enabled": bool(thinking_mode),
                "generation_time": 0.0,
                "note": f"exception:{e}",
                "prompt_config": prompt_meta,
            }
            return err_result, 0.0

    return process_one

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--max_samples",
        type=int,
        default=DEFAULT_MAX_SAMPLES,
        help=f"æœ€å¤§æ ·æœ¬æ•°ï¼ˆé»˜è®¤ {DEFAULT_MAX_SAMPLES}ï¼‰",
    )
    parser.add_argument("--workers", type=int, default=DEFAULT_WORKERS)
    parser.add_argument("--thinking", action="store_true", help="å¯ç”¨ thinking æ¨¡å¼ï¼ˆè¦†ç›–å…¨å±€ï¼‰")
    parser.add_argument("--temperature", type=float, default=DEFAULT_TEMPERATURE)
    parser.add_argument("--save_raw", action="store_true", help="ä¿å­˜ä¸€æ¬¡ raw responseï¼ˆå« messagesï¼‰")

    parser.add_argument(
        "--exp_family",
        type=str,
        default=DEFAULT_EXP_FAMILY,
        choices=["none", "quality", "complexity", "num_demos", "diversity"],
        help="Prompt å®éªŒç»´åº¦ï¼šnone / quality / complexity / num_demos / diversity",
    )
    parser.add_argument(
        "--condition",
        type=str,
        default=DEFAULT_CONDITION,
        help="å…·ä½“ condition åç§°ï¼Œä¾‹å¦‚ï¼šclean / wrong_demo / simple / detailed ç­‰",
    )
    parser.add_argument(
        "--num_demos",
        type=int,
        default=DEFAULT_NUM_DEMOS,
        help="ç¤ºä¾‹æ•°é‡ï¼ˆåªåœ¨ exp_family=num_demos æ—¶ç”Ÿæ•ˆï¼‰",
    )
    parser.add_argument(
        "--diversity_mode",
        type=str,
        default=DEFAULT_DIVERSITY_MODE,
        choices=["low", "high"],
        help="Prompt å¤šæ ·æ€§æ¨¡å¼ï¼ˆåªåœ¨ exp_family=diversity æ—¶æœ‰æ„ä¹‰ï¼‰",
    )
    parser.add_argument(
        "--problem_file",
        type=str,
        default=DEFAULT_PROBLEM_FILE,
        help=f"HumanEval é¢˜ç›®æ–‡ä»¶ï¼ˆé»˜è®¤ {DEFAULT_PROBLEM_FILE}ï¼‰",
    )
    parser.add_argument(
        "--output_file",
        type=str,
        default=DEFAULT_OUTPUT_FILE,
        help=f"ç”Ÿæˆç»“æœè¾“å‡ºæ–‡ä»¶ï¼ˆé»˜è®¤ {DEFAULT_OUTPUT_FILE}ï¼‰",
    )

    args = parser.parse_args()

    thinking_mode = args.thinking or GLOBAL_THINKING_MODE

    problems = read_problems(args.problem_file)
    items = list(problems.items())
    if args.max_samples:
        items = items[: args.max_samples]

    task_order = [tid for (tid, _) in sorted(problems.items(), key=lambda x: x[0])]

    print(
        f"ğŸš€ Baseline å¯åŠ¨ï¼š samples={len(items)} "
        f"model={MODEL_NAME} THINKING_MODE={thinking_mode} workers={args.workers}"
    )
    print(
        f"    exp_family={args.exp_family} condition={args.condition} "
        f"num_demos={args.num_demos} diversity_mode={args.diversity_mode}"
    )
    print(f"    problem_file={args.problem_file}  output_file={args.output_file}")

    save_raw_flag: Optional[Dict[str, Any]] = {"saved": False} if args.save_raw else None

    prompt_cfg: Dict[str, Any] = {
        "exp_family": args.exp_family,
        "condition": args.condition,
        "num_demos": args.num_demos,
        "diversity_mode": args.diversity_mode,
        "task_order": task_order,
    }

    process_one = make_process_one(
        MODEL_NAME,
        args.temperature,
        thinking_mode,
        save_raw_flag,
        problems,
        prompt_cfg,
    )

    results_map: Dict[str, Dict[str, Any]] = {}

    with ThreadPoolExecutor(max_workers=args.workers) as ex:
        futures = {ex.submit(process_one, entry): entry for entry in items}
        for fut in tqdm(as_completed(futures), total=len(futures), desc="Generating"):
            entry = futures[fut]
            task_id = entry[0]
            try:
                res, elapsed = fut.result()
            except Exception as e:
                res = {
                    "task_id": task_id,
                    "completion": "",
                    "tokens": {
                        "prompt_tokens": 0,
                        "completion_tokens": 0,
                        "total_tokens": 0,
                    },
                    "thinking_mode_enabled": thinking_mode,
                    "generation_time": 0.0,
                    "note": f"exception_in_future:{e}",
                    "prompt_config": {
                        "exp_family": args.exp_family,
                        "condition": args.condition,
                        "prompt_variant": "future_exception",
                        "num_demos": args.num_demos,
                        "diversity_mode": args.diversity_mode,
                        "prompt_template_id": 0,
                    },
                }
                elapsed = 0.0
            results_map[res["task_id"]] = res

    ordered: List[Dict[str, Any]] = []
    for (task_id, _) in items:
        if task_id in results_map:
            ordered.append(results_map[task_id])
        else:
            ordered.append(
                {
                    "task_id": task_id,
                    "completion": "",
                    "tokens": {
                        "prompt_tokens": 0,
                        "completion_tokens": 0,
                        "total_tokens": 0,
                    },
                    "thinking_mode_enabled": thinking_mode,
                    "generation_time": 0.0,
                    "note": "missing",
                    "prompt_config": {
                        "exp_family": args.exp_family,
                        "condition": args.condition,
                        "prompt_variant": "missing",
                        "num_demos": args.num_demos,
                        "diversity_mode": args.diversity_mode,
                        "prompt_template_id": 0,
                    },
                }
            )

    write_jsonl(args.output_file, ordered)

    valid = [r for r in ordered if r["tokens"].get("total_tokens", 0) > 0]
    n_valid = max(1, len(valid))
    prompt_tot = sum(r["tokens"].get("prompt_tokens", 0) for r in ordered)
    comp_tot = sum(r["tokens"].get("completion_tokens", 0) for r in ordered)
    total_tot = sum(r["tokens"].get("total_tokens", 0) for r in ordered)
    model_total_runtime = sum(float(r.get("generation_time", 0.0) or 0.0) for r in ordered)
    avg_gen_time = model_total_runtime / n_valid

    print("\nğŸ“Š Baseline ç»Ÿè®¡ï¼š")
    print(f"æ ·æœ¬æ•°: {len(ordered)}  æœ‰æ•ˆæ ·æœ¬: {n_valid}")
    print(f"å¹³å‡è¾“å…¥ tokens: {prompt_tot / n_valid:.2f}")
    print(f"å¹³å‡è¾“å‡º tokens: {comp_tot / n_valid:.2f}")
    print(f"å¹³å‡æ€» tokens: {total_tot / n_valid:.2f}")
    print(f"å¹³å‡ç”Ÿæˆè€—æ—¶(æ¨¡å‹ç«¯): {avg_gen_time:.3f} s")
    print(f"æ¨¡å‹æ€»è¿è¡Œæ—¶é•¿: {model_total_runtime:.3f} s")
    print(f"ç»“æœå·²ä¿å­˜åˆ° {args.output_file}")
    print(
        f"è¯„ä¼°å‘½ä»¤ï¼špython evaluate_functional_correctness.py "
        f"--sample_file {args.output_file} --problem_file {args.problem_file}"
    )

if __name__ == "__main__":
    main()