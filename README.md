# COMP7607 Assignment 2: Analysis of Prompting Strategies for Code Generation

[English](https://github.com/SeanLIUXQ/COMP7607_Assignment2?tab=readme-ov-file#english) | [ä¸­æ–‡](https://github.com/SeanLIUXQ/COMP7607_Assignment2?tab=readme-ov-file#%E4%B8%AD%E6%96%87https://www.google.com/search?q=%23ä¸­æ–‡)

------

## ä¸­æ–‡

### ğŸ“‹ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ˜¯ **COMP7607 Natural Language Processing (Fall 2025)** è¯¾ç¨‹ Assignment 2 çš„å®ç°ä»£ç ä¸åˆ†ææŠ¥å‘Š ã€‚

æœ¬é¡¹ç›®æ—¨åœ¨æ·±å…¥æ¢ç©¶ **æç¤ºå·¥ç¨‹ï¼ˆPrompt Engineeringï¼‰** å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸Šæ€§èƒ½çš„å½±å“ã€‚å®éªŒåŸºäº **HumanEval** åŸºå‡†æµ‹è¯•é›†ï¼Œä½¿ç”¨äº† **Qwen3-8B** æ¨¡å‹ï¼Œåˆ†æäº†æç¤ºè¯è´¨é‡ã€å¤æ‚åº¦ã€ç¤ºä¾‹æ•°é‡ï¼ˆFew-shotï¼‰ä»¥åŠå¤šæ ·æ€§å¯¹ä»£ç ç”Ÿæˆå‡†ç¡®ç‡ï¼ˆPass@1ï¼‰çš„å½±å“ã€‚

æ­¤å¤–ï¼Œæœ¬é¡¹ç›®è¿˜å¯¹æ¯”äº†ä¸¤ç§æ¨ç†ç­–ç•¥ï¼š

1. **Baseline Method**ï¼šç›´æ¥åŸºäºæç¤ºè¯ç”Ÿæˆä»£ç ã€‚
2. **Combine Method**ï¼šå¼•å…¥äº†è‡ªæˆ‘ä¿®æ­£ï¼ˆSelf-Refineï¼‰å’ŒåŸºäºå•å…ƒæµ‹è¯•åé¦ˆçš„ä¿®å¤å¾ªç¯ï¼ˆCodeT-style repairï¼‰ã€‚

### ğŸ¯ å®éªŒç»´åº¦ (Dimensions)

æ ¹æ®ä½œä¸šè¦æ±‚ï¼Œæœ¬é¡¹ç›®å®ç°äº†é’ˆå¯¹ä»¥ä¸‹å››ä¸ªç»´åº¦çš„å¯¹æ¯”å®éªŒï¼š

1. **Prompt Quality (æç¤ºè¯è´¨é‡)**
   - `clean`: æ ‡å‡†ã€æ­£ç¡®çš„æè¿°ä¸ç¤ºä¾‹ã€‚
   - `wrong_demo`: åŒ…å«æ•…æ„é”™è¯¯çš„ç¤ºä¾‹ä»£ç ã€‚
   - `irrelevant_demo`: åŒ…å«æ­£ç¡®ä½†ä¸å½“å‰ä»»åŠ¡æ— å…³çš„ç¤ºä¾‹ã€‚
   - `bad_instruction`: åŒ…å«è¯¯å¯¼æ€§çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚
2. **Prompt Complexity (æç¤ºè¯å¤æ‚åº¦)**
   - `simple`: æåº¦ç®€åŒ–çš„ä»»åŠ¡æè¿°ã€‚
   - `original`: åŸå§‹ HumanEval æè¿°ã€‚
   - `detailed`: åŒ…å«é¢å¤–çº¦æŸå’Œè¾¹ç•Œæ¡ä»¶çš„è¯¦ç»†æè¿°ã€‚
3. **Number of Demonstrations (ç¤ºä¾‹æ•°é‡)**
   - $k \in \{0, 1, 2, 4\}$ï¼šæ¯”è¾ƒ Zero-shot ä¸ Few-shot çš„æ•ˆæœã€‚
4. **Prompt Diversity (æç¤ºè¯å¤šæ ·æ€§)**
   - `low`: ä½¿ç”¨å›ºå®šæ¨¡æ¿ã€‚
   - `high`: ä½¿ç”¨å¤šç§ä¸åŒå¥å¼å’Œç»“æ„çš„æ¨¡æ¿ã€‚

### ğŸ“ é¡¹ç›®ç»“æ„

```
COMP7607_Assignment2/
â”œâ”€â”€ baseline_eval_results/      # Baseline æ–¹æ³•çš„è¯„æµ‹ç»“æœ (.jsonl)
â”œâ”€â”€ combine_eval_results/       # Combine (Self-refine) æ–¹æ³•çš„è¯„æµ‹ç»“æœ
â”œâ”€â”€ baseline_eval_summaries/    # ç»“æœæ‘˜è¦ç»Ÿè®¡
â”œâ”€â”€ combine_eval_summaries/     # ç»“æœæ‘˜è¦ç»Ÿè®¡
â”œâ”€â”€ main_generate_baseline.py   # Baseline æ–¹æ³•çš„ä¸»ç”Ÿæˆè„šæœ¬
â”œâ”€â”€ method_combine.py           # Combine æ–¹æ³•çš„ä¸»ç”Ÿæˆè„šæœ¬ (Self-refine + Repair)
â”œâ”€â”€ evaluate_functional_correctness.py # åŠŸèƒ½æ­£ç¡®æ€§è¯„ä¼°è„šæœ¬
â”œâ”€â”€ execution.py                # ä»£ç æ‰§è¡Œæ²™ç®±/å·¥å…·
â”œâ”€â”€ HumanEval.jsonl             # æ•°æ®é›†
â”œâ”€â”€ requirements.txt            # Python ä¾èµ–
â””â”€â”€ README.md                   # è¯´æ˜æ–‡æ¡£
```

### ğŸš€ å¿«é€Ÿå¼€å§‹

#### 1. ç¯å¢ƒå‡†å¤‡

ç¡®ä¿æ‚¨çš„ Python ç‰ˆæœ¬ä¸º 3.8+ï¼Œå¹¶å®‰è£…ä¾èµ–ï¼š

Bash

```
pip install -r requirements.txt
```

#### 2. API é…ç½®

æœ¬é¡¹ç›®æ”¯æŒ OpenAI å…¼å®¹æ ¼å¼çš„ APIï¼ˆå¦‚é˜¿é‡Œäº‘ Bailian/DashScopeï¼‰ã€‚è¯·åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½®æ‚¨çš„ API Keyï¼š

Bash

```
# Linux / macOS
export OPENAI_API_KEY="sk-xxxxxxxxxxxxxxxx"
export BASE_URL="https://dashscope.aliyuncs.com/compatible-mode/v1"
export MODEL_NAME="qwen3-8b" # æˆ–å…¶ä»–æ‚¨ä½¿ç”¨çš„æ¨¡å‹

# Windows (PowerShell)
$env:OPENAI_API_KEY="sk-xxxxxxxxxxxxxxxx"
```

### ğŸ’» ä½¿ç”¨æ–¹æ³• (Usage)

è„šæœ¬ `main_generate_baseline.py` æ”¯æŒé€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ§åˆ¶å®éªŒå˜é‡ã€‚

#### é€šç”¨å‚æ•°

- `--exp_family`: å®éªŒç»´åº¦ (`quality`, `complexity`, `num_demos`, `diversity`, `none`)ã€‚
- `--condition`: å…·ä½“æ¡ä»¶ (å¦‚ `clean`, `simple` ç­‰)ã€‚
- `--num_demos`: ç¤ºä¾‹æ•°é‡ (ä»…åœ¨ `num_demos` å®éªŒä¸‹ç”Ÿæ•ˆ)ã€‚
- `--diversity_mode`: å¤šæ ·æ€§æ¨¡å¼ (`low`, `high`)ã€‚
- `--max_samples`: æµ‹è¯•æ ·æœ¬æ•°é‡ (é»˜è®¤ä¸º 80)ã€‚

#### è¿è¡Œç¤ºä¾‹

**1. è¿è¡Œ Baseline é»˜è®¤è®¾ç½® (Original, k=0)**

Bash

```
python main_generate_baseline.py --exp_family none
```

**2. å®éªŒï¼šæç¤ºè¯è´¨é‡ (Prompt Quality)**

Bash

```
# æµ‹è¯•åŒ…å«é”™è¯¯ç¤ºä¾‹çš„æƒ…å†µ
python main_generate_baseline.py --exp_family quality --condition wrong_demo
```

**3. å®éªŒï¼šæç¤ºè¯å¤æ‚åº¦ (Prompt Complexity)**

Bash

```
# æµ‹è¯•ç®€åŒ–æè¿°çš„æƒ…å†µ
python main_generate_baseline.py --exp_family complexity --condition simple
```

**4. å®éªŒï¼šç¤ºä¾‹æ•°é‡ (Number of Demonstrations)**

Bash

```
# 4-shot learning
python main_generate_baseline.py --exp_family num_demos --num_demos 4
```

**5. è¿è¡Œ Combine (Self-Refine) æ–¹æ³•**

*(å‡è®¾ method_combine.py æ¥å—ç±»ä¼¼çš„å‚æ•°ç»“æ„)*

Bash

```
python method_combine.py --exp_family quality --condition clean
```

#### è¯„ä¼°ç»“æœ

ç”Ÿæˆå®Œæˆåï¼Œä½¿ç”¨è¯„ä¼°è„šæœ¬è®¡ç®— Pass@1 å‡†ç¡®ç‡ï¼š

Bash

```
python evaluate_functional_correctness.py --sample_file baseline_eval_results/baseline_A2_default.jsonl
```

### ğŸ“Š å…³é”®ç»“è®º (Key Findings)

åŸºäºå®éªŒæŠ¥å‘Šçš„åˆ†æï¼Œä¸»è¦å‘ç°å¦‚ä¸‹ï¼š

1. **å¤æ‚åº¦è‡³å…³é‡è¦**ï¼šæç¤ºè¯çš„è¯¦ç»†ç¨‹åº¦å¯¹æ€§èƒ½å½±å“æœ€å¤§ã€‚è¿‡åº¦ç®€åŒ–çš„æè¿° (`simple`) ä¼šå¯¼è‡´å‡†ç¡®ç‡å¤§å¹…ä¸‹é™ï¼ˆä» ~87% é™è‡³ ~55%ï¼‰ã€‚
2. **è‡ªæˆ‘ä¿®æ­£çš„æœ‰æ•ˆæ€§**ï¼šCombine æ–¹æ³•ï¼ˆSelf-Correctionï¼‰åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹éƒ½èƒ½æå‡ Baseline çš„æ€§èƒ½ï¼ˆå¹³å‡æå‡çº¦ 5%ï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆå§‹æç¤ºè¯å­˜åœ¨å™ªéŸ³ï¼ˆå¦‚é”™è¯¯ç¤ºä¾‹ï¼‰æ—¶è¡¨ç°å‡ºæ›´å¼ºçš„é²æ£’æ€§ã€‚
3. **ç¤ºä¾‹æ•°é‡çš„è¾¹é™…é€’å‡**ï¼šå¢åŠ ç¤ºä¾‹æ•°é‡ï¼ˆFew-shotï¼‰åœ¨ $k=1$ æ—¶è¾¾åˆ°å³°å€¼ï¼Œç»§ç»­å¢åŠ ç¤ºä¾‹ ($k=2, 4$) å¹¶æ²¡æœ‰å¸¦æ¥æ˜¾è‘—çš„çº¿æ€§æå‡ï¼Œç”šè‡³å¯èƒ½å¼•å…¥å¹²æ‰°ã€‚
4. **å¤šæ ·æ€§å½±å“è¾ƒå°**ï¼šæ”¹å˜æç¤ºè¯çš„å¥å¼å’Œç»“æ„ï¼ˆDiversityï¼‰å¯¹æœ€ç»ˆä»£ç ç”Ÿæˆçš„å‡†ç¡®ç‡å½±å“å¾®ä¹å…¶å¾®ã€‚

### ğŸ‘¤ ä½œè€…

- **Name**: Sean LIU
- **Course**: COMP7607 @ HKU
- **Report**: Analysis of Prompting Strategies for Coding.docx

------

## English

### ğŸ“‹ Project Overview

This repository contains the implementation for **COMP7607 Assignment 2**, focusing on the **Analysis of Prompting Strategies for Coding**.



We explore how different prompt factors affect the reasoning and code generation capabilities of LLMs (specifically **Qwen3-8B**) using the **HumanEval** benchmark. Furthermore, we compare a standard **Baseline** method against a **Combine** method that utilizes self-refinement and test-based repair.

### ğŸ¯ Experimental Dimensions

As per the assignment requirements, we analyze four key dimensions:

1. **Prompt Quality**: `clean`, `wrong_demo`, `irrelevant_demo`, `bad_instruction`.
2. **Prompt Complexity**: `simple`, `original`, `detailed`.
3. **Number of Demonstrations**: $k \in \{0, 1, 2, 4\}$.
4. **Prompt Diversity**: `low` vs. `high`.

### ğŸš€ Getting Started

1. **Install Dependencies**:

   Bash

   ```
   pip install -r requirements.txt
   ```

2. **Set API Key**:

   Bash

   ```
   export OPENAI_API_KEY="your-api-key"
   export MODEL_NAME="qwen3-8b"
   ```

### ğŸ’» Usage

Run the baseline generation script with specific experiment parameters:

Bash

```
# 1. Baseline (Default)
python main_generate_baseline.py --exp_family none

# 2. Experiment: Quality (e.g., Wrong Demo)
python main_generate_baseline.py --exp_family quality --condition wrong_demo

# 3. Experiment: Complexity (e.g., Simple)
python main_generate_baseline.py --exp_family complexity --condition simple

# 4. Experiment: Num Demos (e.g., k=2)
python main_generate_baseline.py --exp_family num_demos --num_demos 2
```

### ğŸ“Š Results Summary

- **Specification Quality dominates**: Simplistic prompts drastically reduce performance.
- **Self-Correction works**: The Combine method consistently improves over the baseline, especially recovering from noisy prompts.
- **Few-shot saturation**: Performance peaks around $k=1$; adding more shots provides diminishing returns.
- **Diversity is secondary**: Paraphrasing prompts has minimal impact compared to content quality.

For full details, please refer to the report: `Analysis of Prompting Strategies for Coding.docx`.
